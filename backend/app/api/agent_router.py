import asyncio
import json
from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from ..orchestor.state_models import PromptRequest, AgentState
from ..orchestor.graph import agent_executor

# Create an instance of APIRouter. This is the object that app.main will import and use.
router = APIRouter()

# Define the endpoint for processing prompts.
# We use .post() because the client is sending data (the prompt) to the server.
@router.post("/process_prompt")
async def process_prompt(request: PromptRequest):
    """
    This endpoint receives a user's prompt and streams back the agent's thought process
    and results in real-time using Server-Sent Events (SSE).

    The stream sends JSON objects with a 'type' and 'data' field, allowing the
    frontend to dynamically update the UI based on the agent's current activity.
    """

    async def event_stream():
        """The generator function that yields events for the streaming response."""
        try:
            # The initial state for the LangGraph agent is the user's original prompt.
            inputs = {"original_prompt": request.prompt}
            
            # Use astream_events to get a detailed, real-time feed of events from the graph.
            # This is more powerful than a simple .stream() or .invoke() as it tells us
            # exactly what's happening inside the agent's "mind".
            async for event in agent_executor.astream_events(inputs, version="v1"):
                kind = event["event"]
                
                # We are primarily interested in the 'on_chain_end' event, which fires
                # whenever a node in our graph finishes its execution.
                if kind == "on_chain_end":
                    node_name = event["name"]  # The name of the node that just finished (e.g., "planner")
                    output = event["data"].get("output")
                    
                    # Skip streaming for the internal router node
                    if node_name == "router":
                        continue

                    # Prepare the data packet to send to the frontend
                    # The 'type' corresponds to the node name, giving the frontend context.
                    # The 'data' is the actual output from that node.
                    response_data = {
                        "type": node_name,
                        "data": output
                    }
                    
                    # Yield the data in the Server-Sent Event format: "data: {json_string}\n\n"
                    # The two newlines are critical for the protocol.
                    yield f"data: {json.dumps(response_data)}\n\n"

        except Exception as e:
            # If any error occurs during the stream, send an error event to the frontend.
            error_data = {
                "type": "error",
                "data": {"message": f"An error occurred in the agent stream: {str(e)}"}
            }
            yield f"data: {json.dumps(error_data)}\n\n"
        
        finally:
            # Signal the end of the stream
            end_data = {
                "type": "stream_end",
                "data": {"message": "Agent processing complete."}
            }
            yield f"data: {json.dumps(end_data)}\n\n"

    # Return a StreamingResponse, which keeps the HTTP connection open and sends
    # data as it's generated by the event_stream generator.
    return StreamingResponse(event_stream(), media_type="text/event-stream")