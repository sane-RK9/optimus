version: '3.9'

services:
  # Your Python Backend Agent Service
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      # Expose the backend's port 8000 to the host's port 8000
      - "8000:8000"
    volumes:
      # Mount local directories into the container for live-reloading and data persistence
      - ./backend/app:/app/app         # Mount your app code for live changes
      - ./data:/app/data               # Mount persistent data
      - ./logs:/app/logs               # Mount logs
      - ./workspace:/app/workspace     # Mount the agent's workspace
    environment:
      # Pass environment variables to the backend
      # It will use these to connect to the other services
      - OLLAMA_HOST=http://ollama:11434
      - CHROMA_HOST=chroma
      - CHROMA_PORT=8000
      - PYTHONUNBUFFERED=1
    depends_on:
      # Ensure ollama and chroma start before the backend
      ollama:
        condition: service_healthy
      chroma:
        condition: service_healthy
    networks:
      - agent_network

  # The Ollama LLM Service
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      # Use a named volume to persist downloaded LLM models
      - ollama_data:/root/.ollama
    deploy:
      # This section is crucial for GPU acceleration
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 # or 'all'
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - agent_network

  # The ChromaDB Vector Store Service
  chroma:
    image: ghcr.io/chroma-core/chroma
    ports:
      # IMPORTANT: Chroma also uses port 8000 by default. We map it to 8001 on the host
      # to avoid conflict with our backend service. The backend will still connect to it on port 8000 internally.
      - "8001:8000"
    volumes:
      # Mount a local directory to persist the vector database
      - ./data/chroma_db:/chroma/chroma
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - agent_network

# Define the network for services to communicate
networks:
  agent_network:
    driver: bridge

# Define the named volume for Ollama models
volumes:
  ollama_data: