# -----------------------------------------------------------------------------
# Local Agent Backend Configuration
#
# This file serves as a template for the .env file.
# Copy this file to .env and fill in the values for your local setup.
# The .env file is git-ignored and should NOT be committed to version control.
# -----------------------------------------------------------------------------

# --- LLM Configuration ---
# The full URL of your locally running Ollama server.
# This is the default value used by the Docker Compose setup.
OLLAMA_HOST="http://127.0.0.1:11434"

# The default model to use for the agent's reasoning.
# Examples: "llama3", "phi3", "mistral", "mixtral"
# Ensure this model is available on your Ollama server.
PLANNER_MODEL =  "llama3.2:3b"
CODER_MODEL =  "codellama:7b"
ROUTER_MODEL = "llama3.2:1b"


# --- ChromaDB (Vector Store) Configuration ---
# The hostname for the ChromaDB service.
# This is the default value used by the Docker Compose setup.
CHROMA_HOST="chroma"

# The port for the ChromaDB service.
CHROMA_PORT="8000"


"

