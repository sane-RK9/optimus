# ğŸš€ Local Agentic Automation System

**A local-first, privacy-respecting desktop application where AI agents autonomously understand natural language prompts to plan, execute, and adapt automation tasks on your machine.**

![Project Banner](https://via.placeholder.com/800x300.png?text=Local+Agentic+Automation+System) <!-- Replace with actual banner later -->

> **Your personal automation assistant that works offline, respects privacy, and adapts to your needs**

## âœ¨ Core Philosophy

|         Principle          |                Description                            |
|----------------------------|-------------------------------------------------------|
|**ğŸ¤– Agentic Autonomy**     | AI agents handle task lifecycle independently        |
|**ğŸ”’ Local-First Privacy**  | All processing stays on your device - no data leaves |
|**ğŸ§© Dynamic Adaptation**   | Code generated on-the-fly for each unique task       |
|**ğŸ›¡ï¸ User-in-the-Loop**     | Explicit consent required for sensitive actions      |
|**ğŸ“± Hardware Accessible**  | Works from basic laptops to powerful workstations    |

## ğŸš€ Key Features

- **Natural Language Interface**: "Remind me to water plants every Tuesday at 5 PM"
- **Self-Improving Memory**: Learns from successful tasks via ChromaDB vector store
- **Tiered Security Model**:
  - AST-based static code analysis
  - Resource-limited sandbox execution
  - Granular consent gates
- **Hardware Optimization**: 
  - Automatic model selection (Phi-3 to Llama 3)
  - GPU acceleration support
- **Cross-Platform**: Native desktop experience for Windows, macOS, Linux

## ğŸ§© Tech Stack Architecture

```mermaid
graph TD
    A[Tauri UI] --> B[Rust Core]
    B --> C[FastAPI Server]
    C --> D[LangGraph Orchestration]
    D --> E[Ollama LLMs]
    D --> F[Code Sandbox]
    D --> G[ChromaDB Memory]
    D --> H[SQLite Logging]
```

## âš™ï¸ System Requirements

### Minimum (Basic Automation)
- CPU: x86-64-v2 (Intel Sandy Bridge+/AMD Bulldozer+)
- RAM: 8GB DDR4
- Storage: 128GB SSD
- Recommended Model: `phi3` (1.8GB)

### Recommended (Advanced Tasks)
- CPU: 6-core/12-thread (Zen2+/Raptor Lake+)
- RAM: 16GB DDR5
- GPU: NVIDIA RTX 3060/AMD RX 6600XT
- Storage: NVMe SSD
- Recommended Model: `llama3:8b` (4.7GB)

## ğŸš€ Getting Started

### 1. Prerequisites
```bash
# Install core dependencies
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh  # Rust
brew install ollama docker node   # macOS
sudo apt install docker-compose nodejs npm  # Linux
```

### 2. Clone & Setup
```bash
git clone https://github.com/your-username/local-automation-agent.git
cd local-automation-agent

# Initialize environment
./setup.sh
```

### 3. Configure Environment
```bash
# Edit .env file for customization
nano .env

# Example configuration:
OLLAMA_MODEL=phi3
SANDBOX_TIMEOUT=30
RISK_THRESHOLD=medium
```

### 4. Launch the System
```bash
# Start backend services in Docker
docker-compose up --build -d

# Launch desktop UI
cd frontend
npm install
npm run tauri dev
```

### 5. Try Your First Automation
Enter in the prompt window:
```text
"Create a daily journal template in Markdown format on my Desktop"
```

## ğŸ“‚ Project Structure Deep Dive

```
local-automation-agent/
â”œâ”€â”€ backend/                  
â”‚   â”œâ”€â”€ app/                  
â”‚   â”‚   â”œâ”€â”€ agent_orchestration/  # LangGraph workflows
â”‚   â”‚   â”‚   â”œâ”€â”€ graph.py          # State machine definition
â”‚   â”‚   â”‚   â”œâ”€â”€ nodes/            # Processing units
â”‚   â”‚   â”‚   â””â”€â”€ state_models.py   # Data structures
â”‚   â”‚   â”œâ”€â”€ services/             
â”‚   â”‚   â”‚   â”œâ”€â”€ sandbox_service.py # Execution environment
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_service.py    # Ollama interface
â”‚   â”‚   â”‚   â””â”€â”€ security.py       # Risk assessment
â”‚   â”‚   â””â”€â”€ api/                  # REST endpoints
â”‚   â””â”€â”€ Dockerfile                # Container configuration
â”œâ”€â”€ frontend/                 
â”‚   â”œâ”€â”€ src/                  # UI components
â”‚   â”‚   â”œâ”€â”€ services/         # Tauri-Rust communication
â”‚   â”‚   â””â”€â”€ stores/           # State management
â”‚   â””â”€â”€ src-tauri/            # Rust core
â”‚       â””â”€â”€ src/              
â”‚           â”œâ”€â”€ commands.rs   # Bridge handlers
â”‚           â””â”€â”€ keychain.rs   # Secure credential storage
â”œâ”€â”€ data/                     # Persistent storage
â”‚   â”œâ”€â”€ chroma/               # Vector database
â”‚   â””â”€â”€ sqlite/               # Task history
â””â”€â”€ workspace/                # Ephemeral execution space
```

## ğŸ”§ Troubleshooting Guide

### Common Issues:
1. **Ollama Connection Failed**:
   ```bash
   docker-compose logs ollama
   curl http://localhost:11434/api/tags # Verify Ollama status
   ```
   
2. **Sandbox Timeout Errors**:
   - Increase `SANDBOX_TIMEOUT` in `.env`
   - Check for infinite loops in generated code

3. **GPU Acceleration Not Working**:
   ```bash
   docker run --gpus all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark
   ```
   Verify NVIDIA toolkit installation

### Performance Tuning:
```bash
# For low-RAM systems
export OLLAMA_MODEL=phi3:4k-q4
export SANDBOX_MEMORY_LIMIT=512

# Enable GPU acceleration
export LLAMA_CUDA=1
```

## ğŸŒŸ Sample Automations

### File Management
```text
"Find all .tmp files in Downloads older than 30 days and delete them"
```

### Data Processing
```text
"Convert the sales-data.csv in Documents to a pivot table Excel file"
```

### Personal Productivity
```text
"Create calendar events for all meetings mentioned in my email from yesterday"
```

## ğŸ—ºï¸ Development Roadmap

|     Phase     | Features                               | Status         |
|---------------|----------------------------------------|----------------|
| **Phase 1**   | Core NLP-to-Code pipeline              | âœ… Released    |
| **Phase 1.5** | Consent gates & Memory system          | ğŸš§ In Progress |
| **Phase 2**   | OS keychain integration & Self-healing | Planned        |
| **Phase 3**   | Hardware-accelerated sandboxing        | Research       |

## ğŸ¤ Contributing Guide

We welcome contributions! Please follow these steps:

1. **Fork** the repository
2. Create a feature branch: 
   ```bash 
   git checkout -b feat/amazing-feature
   ```
3. **Test** your changes:
   ```bash
   cd backend && pytest -v
   cd ../frontend && npm test
   ```
4. Submit a **Pull Request** with:
   - Description of changes
   - Screenshots of UI updates
   - Performance impact analysis

> **Before contributing**: Read our [Security Guidelines](SECURITY.md) and [Code of Conduct](CODE_OF_CONDUCT.md)

## ğŸ“œ License
MIT Licensed - See [LICENSE.md](LICENSE) for details

---

**Made with â¤ï¸ by the Local Automation Team** | [Documentation](docs/) | [Report Issue](issues/new)
